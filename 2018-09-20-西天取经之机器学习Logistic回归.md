---
layout: post
title:  "西天取经之机器学习Logistic回归"
date: 2018-09-21
categories: MachineLearning
tags: MachineLearning
---


#### Logistic回归

前面提到了线性回归，笔记记得一塌糊涂，接下来我们继续瞎扯罗辑回归，对于分类问题，线性方程没法很好的fit在数据上。

二元分类问题中，因为要预测的变量y是离散的值，而非线形的。我们将预测的结果做一个归属，是正类（positive class）还是负类（negative class），即因变量y∈{0,1}。

逻辑回归模型：

>𝒉𝜽(𝚡) = 𝒈(𝜽ᐪ𝘟)
>𝒉𝜽(𝚡)= 1 / (1 + 𝑒⁻ᙆ)

𝒉𝜽(𝚡) 的作用是给定输入变量X，给出选定的参数的输出变量y=1的概率是多少既

> 𝒉𝜽(𝚡) = 𝑃 (y=1|x;𝜽)

正类和负类的和为1 𝑃 (y=0|x;𝜽) + 𝑃 (y=1|x;𝜽) = 1

$ \theta ^2 $ 
$$\sum_{i=0}^N\int_{a}^{b}g(t,i)\text{d}t$$

决策边界(decision boundary)：
在逻辑回归中，我们预测：
当𝒉𝜽(𝚡)> 0.5 预测y=1；
当𝒉𝜽(𝚡)< 0.5 预测y=0；
当 z = 0 时 𝒈(z) = 0.5；
  z < 0 时 𝒈(z) < 0.5；
  
#### Cost Function

对于线性回归的代价函数就是均方误差的和。将逻辑回归的模型代入之后存在一个问题就是代价函数将是一个非凸函数，从函数图像可以看出会存在很多个局部最优解。
线性回归的代价函数为: xxx
逻辑回归的代价函数为：
<a href="https://www.codecogs.com/eqnedit.php?latex=J(\theta)&space;=&space;\frac{1}{m}&space;\sum_{i=1}^{m}{Cost(h\theta(X^{(i)},y^{(i)})}" target="_blank"><img src="https://latex.codecogs.com/png.latex?J(\theta)&space;=&space;\frac{1}{m}&space;\sum_{i=1}^{m}{Cost(h\theta(X^{(i)},y^{(i)})}" title="J(\theta) = \frac{1}{m} \sum_{i=1}^{m}{Cost(h\theta(X^{(i)},y^{(i)})}" /></a>

<a href="https://www.codecogs.com/eqnedit.php?latex={Cost(h\theta(X^{(i)},y^{(i)})}&space;=\begin{pmatrix}&space;-log(h\theta(X))&space;if&space;y&space;=&space;1&space;\\&space;-log(1-h\theta(X))&space;if&space;y&space;=0&space;\end{pmatrix}" target="_blank"><img src="https://latex.codecogs.com/png.latex?{Cost(h\theta(X^{(i)},y^{(i)})}&space;=\begin{pmatrix}&space;-log(h\theta(X))&space;if&space;y&space;=&space;1&space;\\&space;-log(1-h\theta(X))&space;if&space;y&space;=0&space;\end{pmatrix}" title="{Cost(h\theta(X^{(i)},y^{(i)})} =\begin{pmatrix} -log(h\theta(X)) if y = 1 \\ -log(1-h\theta(X)) if y =0 \end{pmatrix}" /></a>
