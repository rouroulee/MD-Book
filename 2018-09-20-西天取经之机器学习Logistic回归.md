---
layout: post
title:  "西天取经之机器学习Logistic回归"
date: 2018-09-21
categories: MachineLearning
tags: MachineLearning
---


## Logistic回归

前面提到了线性回归，笔记记得一塌糊涂，接下来我们继续瞎扯罗辑回归，对于分类问题，线性方程没法很好的fit在数据上。

二元分类问题中，因为要预测的变量y是离散的值，而非线形的。我们将预测的结果做一个归属，是正类（positive class）还是负类（negative class），即因变量y∈{0,1}。

逻辑回归模型：

$$ 𝒉𝜽(𝚡) = 𝒈(𝜽ᐪ𝘟)$$
$$ 𝒉𝜽(𝚡)= \frac{1}{1 + 𝑒⁻ᙆ}$$

𝒉𝜽(𝚡) 的作用是给定输入变量X，给出选定的参数的输出变量y=1的概率是多少既

$$ 𝒉𝜽(𝚡) = 𝑃 (y=1|x;𝜽) $$

正类和负类的和为1
$$ 𝑃 (y=0|x;𝜽) + 𝑃 (y=1|x;𝜽) = 1$$


决策边界(decision boundary)：
在逻辑回归中，我们预测：
当𝒉𝜽(𝚡)> 0.5 预测y=1；
当𝒉𝜽(𝚡)< 0.5 预测y=0；
当 z = 0 时 𝒈(z) = 0.5；
  z < 0 时 𝒈(z) < 0.5；
  
## Cost Function

对于线性回归的代价函数就是均方误差的和。将逻辑回归的模型代入之后存在一个问题就是代价函数将是一个非凸函数，从函数图像可以看出会存在很多个局部最优解。
线性回归的代价函数为: xxx
逻辑回归的代价函数为：

$$ J(\theta) = \frac{1}{m} \sum_{i=1}^mCost(h_\theta(X^{(i)},y^{(i)}))$$

$$ Cost(h_\theta(X^{(i)},y^{(i)})) = \begin{cases}
-log(h_\theta(X)) ,  & \text{ if $y$ = 1 } \\
-log(1 - h_\theta(X)),  & \text{ if $y$ = 0 }
\end{cases}
$$


## regularization（正则化）

线性回归和逻辑回归应用到机器学习的场景中，往往会出现过拟合现象（overfitting），正则化则是为了解决这一问题，以达到算法更好的实现。

过拟合一般发生在特征数量过多的情况下，这个时候训练出的方程总能拟合训练数据，代价函数J(𝜽)的值也非常接近于0。这样的参数的方程在用到测试集的时候结果会很差，泛化能力会很弱。

解决过拟合问题的方法有俩类：
Option1:   
-尽量减少选取变量的数量。
    1.人工选择没有必要的变量。      
    2.让模型自主的选择减少不必要的变量。    
Option2:    
-正则化  
    1.保留所有的特征变量，但是减小参数的大小$\theta_j$。  

假设我们有预测函数 $$ 𝒉_𝜽(𝚡) = 𝜽_0 + 𝜽_1x + 𝜽_2x^2 + 𝜽_3x^3 + 𝜽_4x^4 $$

假设函数由于有过多的变量x³、x⁴影响了拟合数据，我们要减小x³、x⁴的影响，加入惩罚项，Cost Function
&&min_\theta\frac{1}{2m}\sum{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2 + \lambda\sum{j=1}^n\theta_j^2 &&











