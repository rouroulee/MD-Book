---
layout: post
title:  "西天取经之机器学习CostFunction & Gradient Descent"
date: 2018-09-10
categories: MachineLearning
tags: MachineLearning
---


## Cost Function


We are meansure the accuracy of our hypothesis function by using a `Cost Function`. This takes an average difference of all results of the hypothesis input from x's and the actual output y's.
<img src="https://wx3.sinaimg.cn/mw690/006Etwvfly1fv4kh77h8ij30nq050wer.jpg"/>

Cost Function (代价函数)又叫做 平方误差函数或者均方误差

<img src="https://wx3.sinaimg.cn/mw690/006Etwvfly1fv4kh77wi5j30i60aatak.jpg"/>

## gradient descent

`gradient descent[梯度下降]` 

以单个参数为例的梯度下降公式  θo:= θo - α * σ/ (σθo)*J(θo)  
`θo` 参数

` α ` 学习率

`(σ/σθo)*J(θo) `  导数项

单个参数就是我们有较好只觉得线性函数，梯度下降求的最红minimize是函数上导数为0的那个点，即切线是水平线。
俩个参数即一个三维坐标的一个面，梯度下降求的最终是minimize是这个面最低的点（下山例子求最短路径），
 θ0，θ1 可以是很多个参数，参数变多的情况，我们无法用直观的画图来表示变化。

Gradient descent 的目的是寻找最优的参数θ
gradient descent通过（simultaneously update）同步更新 参数 来寻找minimize，每一次向梯度下降的方向移动的步（step）大小是由`Learning Rate[学习率]` 来决定的。

## 预测函数 Hypothesis Function
例如房价的假设函数：<i>h</i>θ(х) =  θ₀ +  θ₁ X<sub>1</sub>
这个时候影响房价的特征只有一个X

#### Multiple Features
实际情况是影响房价的特征有多个：面积、卧室的数量、年龄等
这个时候<em> h(x)</em> :
<i>h</i>θ(х) =  θ₀ +  θ₁ X<sub>1</sub> + θ₂X<sub>2</sub> 

> m = training set 样本的数量

> n = training set 样本特征的数量

> X<sup>i</sup> = 第i个样本

> X<sub>j</sub><sup>(i)</sub> ：        第i个样本的第j个特征

## 提升梯度下降的速度的方法

### Feature scaling
<B>Feature scaling[特征缩放]</B>  ：可以有效的缩小梯度下降的时间。

讲输入值除以输入范围的大小【例如X<sub>1</sub> 房子的Size 在1~2000之间 X<sub>2</sub>房间的个数在1~5之间 】特征缩放之后：X<sub>1</sub> / 2000 和 X<sub>2</sub> / 5 都保持在一个-1 < X<sub>i</sub> <+1 。
这样可以保证θ收敛的速度快一点。
### mean normalization 
<B>mean normalization [均值归一化]</B> ：可以有效的缩小梯度下降的时间。
X₁ =（ X₁ - μ ）/ S 
其中 μ 表示 X₁特征范围的平均值 S 表示特征值最大值减去最小值得范围差 

例如：表示房屋价格在100到2000之间，平均值为1000，那么，X₁ = (X₁ - 1000) / 1900

## Normal Equation
`Normal Equation[正规化方程]` θ = (XᐪX)⁻¹ Xᐪy  

 
 |Gradient Descent|Normal Equation|
 |--|--|
 |Need to choose alpha|No Need to choose alpha|



#### Gradient descent or Normal Equation ?
1.特征数如果少（<10000）建议用正规化方程，直接求出代价函数J(θ)的最小指。如果特征很多，还是用梯度下降。
2.正规化方程没有局部最优
3.解决矩阵不可逆，除非减少特征数量n比m少


